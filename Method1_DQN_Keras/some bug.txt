版本说明：
这个版本实现了每一次训练都要重新计算时间窗的RL训练过程。

1、每一个epoch结束以后，没有把那些orbit的参数、整个的参数初始化。#解决了
2、在test的过程中会出现重复的任务，可能是没有控制好不能选重复任务的条件。#什么都没干就解决了……
3、为什么每一次调度只能成功20几个任务？这个可以理解，因为训练只用了一组场景。但是为什么训练的时候也只有300多？这个肯定有问题！！

！！！重磅发现！算出来的时间窗，不一定在阳照区内！！！所以实际上可以选的任务并不多。最终结果是这样的，就是因为这一天之内没有阳照区的时间窗，所以有些任务没办法做。

游泳的时候想到的：
1、如果一个任务已经不存在可用的时间窗，或者已经被调度了，那么可以考虑把state中的这个任务删掉，这样学习任务数量小于max的时候会效率更高！

AlphaGo第一篇:
1、实验要设计不同的网络结构：1隐含层、2隐含层、3隐含层，每层100个、每层200个、每层300个神经元各自的区别；

0317：

现在train的过程应该是没问题了，test会陷入死循环，周一改好！。orbittasks不会减少，而且orbitresult也不更新。导致一直无法结束该轨。